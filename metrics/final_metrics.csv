"Metric Name","Component","What it Measures","Why it Matters","How to Implement","Other Information (Calculation, Range, Tools)"
"Retrieval Metrics","Retrieval","Assess the quality of the documents/context fetched by the retriever.","","",""
"Context Precision","Retrieval","Relevance of retrieved documents to the query [[4]][[8]]","Ensures the retriever avoids irrelevant or redundant information","Use Ragas’ `context_precision` or manual relevance scoring","Calculated as: (Number of relevant retrieved chunks) / (Total number of retrieved chunks). Often measured at K (e.g., Precision@K). Range [0, 1]. Tools like RAGAs, RagaAI can measure this."
"Context Recall","Retrieval","Coverage of all relevant information in retrieved docs [[4]][[8]]","Identifies gaps in retrieval (e.g., missing key figures like £349 million)","Ragas’ `context_recall` (requires ground-truth references)","Calculated as: (Number of relevant retrieved chunks) / (Total number of relevant chunks in the knowledge source). Requires ground truth or a proxy (like ground truth answer). Range [0, 1]. Tools like RAGAs can measure this."
"Precision@k","Retrieval","% of top-k retrieved docs that are relevant [[6]]","Prioritizes high-quality retrieval results","Tools like Pyserini or Hugging Face Evaluate","Calculated as: (Number of relevant items in top K) / K. Range [0, 1]."
"Recall@k","Retrieval","% of total relevant docs retrieved in top-k [[6]]","Ensures critical information isn’t missed","Pyserini or custom benchmarks","Calculated as: (Number of relevant items in top K) / (Total relevant items). Range [0, 1]."
"MRR (Mean Reciprocal Rank)","Retrieval","Rank of the first relevant document retrieved [[3]]","Rewards systems that surface relevant docs earlier","Custom script (e.g., `1/rank` of first relevant doc)","Calculated as the average of 1/rank for the first relevant document across queries. Higher is better (max 1). Sensitive to the rank of the first correct item."
"NDCG (Normalized Discounted Cumulative Gain)","Retrieval","Graded relevance of retrieved docs [[9]]","Prioritizes nuanced relevance (e.g., exact vs. partial matches)","Galileo AI or custom implementation","The quality of the ranking of retrieved documents, considering both relevance scores and position. Uses graded relevance scores and discounts documents at lower ranks. Range [0, 1]."
"Generation Metrics","Generation","Assess the quality of the final answer generated by the LLM using the retrieved context.","","",""
"Faithfulness / Groundedness / Attributability","Generation","Whether the answer is factually consistent with the provided context.","Crucial for preventing hallucinations (making things up) and building trust.","Ragas’ `faithfulness` or DeepEval","Checks if claims in the answer can be verified against the context. Often uses LLM-as-judge. Score often [0, 1]. Tools: RAGAs, DeepEval, IBM Watsonx, Vellum AI."
"Answer Relevancy","Generation","How well the answer addresses the query [[4]][[8]]","Ensures answers aren’t vague or off-topic","Ragas’ `answer_relevancy` or LLM-as-a-judge","Assessed independently of faithfulness (an answer can be faithful to context but irrelevant to the query). Often uses LLM-as-judge. Score often [0, 1]. Tools: RAGAs, DeepEval, IBM Watsonx, Vellum AI."
"BLEU/ROUGE","Generation","N-gram overlap with reference answers [[2]]","Measures factual alignment (e.g., ""£349 million"" vs. generated text)","NLTK or Hugging Face Evaluate","Traditional NLP metrics for text similarity. Can indicate textual overlap but may penalize valid paraphrasing or different structures. Use with caution in RAG; faithfulness/relevance often more critical. Score [0, 1] or 0-100."
"Hallucination Detection","Generation","Presence of ungrounded claims in the answer [[7]][[10]]","Critical for trust in sensitive domains (e.g., finance)","DeepEval or custom LLM prompts","Techniques include using LLM-as-judge or specialized models. Often measured as a score or binary detection. Related to Faithfulness."
"Hybrid Metrics","Hybrid/End-to-End","","","",""
"Semantic Answer Similarity (SAS)","Hybrid/End-to-End","Semantic alignment between generated and reference answers [[4]][[8]]","Captures meaning beyond keywords (e.g., ""£349m"" ≈ ""£349 million"")","Embedding-based similarity (e.g., Sentence Transformers)","Often calculated using embeddings (e.g., sentence transformers) and cosine similarity. Range typically [-1, 1] or [0, 1]."
"System-Level Metrics","System-Level","","","",""
"Latency","System-Level/End-to-End","Time taken for retrieval/generation [[9]]","Balances performance with user experience (e.g., keyword search took 4.8s)","Track `execution_time` logs","Measured in seconds or milliseconds. Includes time for retrieval, processing, and generation."
"User Feedback / Satisfaction","System-Level/End-to-End","Real-world satisfaction (e.g., thumbs-up/down) [[4]][[9]]","Grounds metrics in actual user needs","Surveys or analytics tools","Collected via mechanisms like thumbs up/down, star ratings, explicit feedback forms. Provides qualitative and quantitative insights."
"DORA Metrics","System-Level","DevOps performance (e.g., deployment frequency, failure rate) [[7]][[9]]","Measures system reliability and agility","Monitoring tools like Prometheus","Includes metrics like Deployment Frequency, Lead Time for Changes, Change Failure Rate, and Time to Restore Service. Focuses on software delivery and operational performance."
"Context Relevance","Retrieval","How relevant the content of the retrieved context is for answering the query.","Ensures the information retrieved is actually useful for formulating the answer.","Manual scoring or using LLM-as-judge","Similar to precision, but can involve deeper semantic checks on the content itself. Range [0, 1]. Mentioned by Microsoft Learn, RAGAs."
"Hit Rate","Retrieval","Whether at least one relevant document was retrieved (often within the top K results).","A basic check to see if the retriever found anything useful at all.","Measure if rank of first relevant doc is <= K. Can use custom scripts or evaluation frameworks.","Typically calculated as the fraction of queries for which at least one relevant document was retrieved. Binary outcome per query (hit/miss)."
"Answer Correctness","Generation","How factually accurate the answer is compared to ground truth or known facts.","Measures the ultimate correctness, independent of context (context itself could be flawed).","Requires comparison to a ground truth reference answer. Manual evaluation or test suites.","Can be binary (Correct/Incorrect) or graded. Tools: Vellum AI. Requires reliable ground truth."
"Fluency / Coherence","Generation","The linguistic quality of the answer (grammar, readability, naturalness, flow).","Important for user experience and understandability.","Human evaluation, LLM-as-a-judge, or linguistic analysis tools.","Assesses the readability and natural flow of the generated text. Subjective to evaluate without human judgment."