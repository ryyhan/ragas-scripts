Metric Name,Component,What it Measures,Why it Matters,"Other Information (Calculation, Range, Tools)"
Retrieval Metrics,,Assess the quality of the documents/context fetched by the retriever.,,
Context Precision,Retrieval,The proportion of retrieved documents (context chunks) that are relevant to the query.,Measures the signal-to-noise ratio. High precision reduces irrelevant information passed to the generator.,"Calculated as: (Number of relevant retrieved chunks) / (Total number of retrieved chunks). Often measured at K (e.g., Precision@K). Range [0, 1]. Tools like RAGAs, RagaAI can measure this."
Context Recall,Retrieval,The proportion of all relevant documents available in the knowledge source that were retrieved.,Measures how well the retriever finds all the necessary information needed to answer the query.,"Calculated as: (Number of relevant retrieved chunks) / (Total number of relevant chunks in the knowledge source). Requires ground truth or a proxy (like ground truth answer). Range [0, 1]. Tools like RAGAs can measure this."
Context Relevance,Retrieval,How relevant the content of the retrieved context is for answering the query.,Ensures the information retrieved is actually useful for formulating the answer.,"Similar to precision, but can involve deeper semantic checks (e.g., using LLM-as-judge) on the content itself. Range [0, 1]. Mentioned by Microsoft Learn, RAGAs."
Hit Rate,Retrieval,Whether at least one relevant document was retrieved (often within the top K results).,A basic check to see if the retriever found anything useful at all.,Typically calculated as the fraction of queries for which at least one relevant document was retrieved.
Mean Reciprocal Rank (MRR),Retrieval,The average inverse rank of the first relevant document retrieved across multiple queries.,Measures how quickly/easily the system finds the first useful piece of information.,Calculated as the average of 1/rank for the first relevant document. Higher is better (max 1). Sensitive to the rank of the first correct item.
NDCG (Normalized Discounted Cumulative Gain),Retrieval,"The quality of the ranking of retrieved documents, considering both relevance scores and position.","Rewards highly relevant documents being ranked higher than less relevant ones, providing a nuanced view of ranking quality.","Uses graded relevance scores and discounts documents at lower ranks. Range [0, 1]."
Generation Metrics,,Assess the quality of the final answer generated by the LLM using the retrieved context.,,
Faithfulness / Groundedness / Attributability,Generation,Whether the generated answer is factually consistent with the provided context.,Crucial for preventing hallucinations (making things up) and building trust.,"Checks if claims in the answer can be verified against the context. Often uses LLM-as-judge. Score often [0, 1]. Tools: RAGAs, DeepEval, IBM Watsonx, Vellum AI."
Answer Relevance,Generation,How relevant and pertinent the generated answer is to the original user query.,"Ensures the answer directly addresses the user's question, isn't incomplete, or off-topic.","Assessed independently of faithfulness (an answer can be faithful to context but irrelevant to the query). Often uses LLM-as-judge. Score often [0, 1]. Tools: RAGAs, DeepEval, IBM Watsonx, Vellum AI."
Answer Correctness,Generation,How factually accurate the answer is compared to ground truth or known facts.,"Measures the ultimate correctness, independent of context (context itself could be flawed).",Requires a ground truth reference answer or fact checking. Can be binary (Correct/Incorrect) or graded. Tools: Vellum AI.
Fluency / Coherence,Generation,"The linguistic quality of the answer (grammar, readability, naturalness, flow).",Important for user experience and understandability.,Often assessed via human evaluation or using LLM-as-judge comparing against linguistic quality criteria.
End-to-End & Other Metrics,,Assess overall performance or specific aspects beyond core retrieval/generation.,,
Latency,End-to-End,The total time taken from receiving the query to providing the final answer.,"Critical for user experience, especially in real-time applications.","Measured in seconds or milliseconds. Includes time for retrieval, processing, and generation."
User Feedback / Satisfaction,End-to-End,Direct ratings or feedback from users on the quality and usefulness of the answers.,The ultimate measure of whether the system is meeting user needs effectively.,"Collected via mechanisms like thumbs up/down, star ratings, explicit feedback forms."
Semantic Similarity,End-to-End,Meaning-based similarity between the generated answer and a reference answer/context.,"Goes beyond keyword overlap to capture if the meaning is preserved, even with different phrasing.","Often calculated using embeddings (e.g., sentence transformers) and cosine similarity. Range typically [-1, 1] or [0, 1]."
ROUGE / BLEU,End-to-End,N-gram overlap between the generated answer and a reference (ground truth) answer.,Traditional NLP metrics for text similarity.,"Can indicate textual overlap but may penalize valid paraphrasing or different structures. Use with caution in RAG; faithfulness/relevance often more critical. Score [0, 1] or 0-100."